# 语言模型原理

> 随着各种GPT模型系统的火爆，NLP又一次广泛进入人们的视野，也是NLP更加普及的一个大好时机，然而很多人其实并不了解这些基础的概念，不知道什么是人工智能、机器学习、神经网络，更不清楚其关系，随着逐渐学习，深入了解，下面将学习过程中的一些笔记简单整理后分享给大家。
```
把神经网络当成一个万能函数拟合器。

把卷积理解为滤波和特征提取。

把全连接层理解为加权求和。

把反向传播理解为从最后一层逐步调整权重参数。

把设计各种模型当成搭积木。
```
人工智能：通过一切技术手段来模拟高级生物人类的智慧的技术集合。

机器学习：通过数学、统计学等方法论模拟人类学习的过程，来赋予计算机算法模型一定的处理特定类型任务能力的技术，目前流行的机器学习算法包括：决策树、朴素贝叶斯、支持向量机、随机森林、人工神经网络、深度学习等

深度学习神经网络：通过模拟生物大脑构造使用数学函数在计算机上模拟神经元、神经突触的结构和功能来模拟生物大脑的结构和功能的一种结构，一般组成包括输入层、隐藏层、输出层，神经网络和一般算法之间最大的区别是其不可解释性，一个神经网络训练出来后给定输入他会给你输出特定或者符合特定条件的输出，内部的执行逻辑相对传统算法来说是不可解释的，你不知道他内部具体做了什么。

上述三者的关系为：人工智能包括机器学习，机器学习包括深度学习，深度学习的主要手段为神经网络

可解释性：机器学习模型可解释性主要分为内部可解释性和事后可解释性，内部可解释性要求限制模型复杂性，事后可解释性则要求模型训练后分析模型结果（例如和具有内部可解释性的相对基础的模型效果进行对比）

什么是语言模型？为了能够量化地衡量哪个句子更像一句人话，学者设计了一个函数，函数P的思想是根据句子里面前面的一系列前导单词预测后面跟哪个单词的概率大小（理论上除了上文之外，也可以引入单词的下文联合起来预测单词出现概率）。句子里面每个单词都有个根据上文预测自己的过程，把所有这些单词的产生概率乘起来，数值越大代表这越像一句人话。

经典的NNLM模型（2003年论文发布，2013年大火）通过让神经网络不断学习下面的函数，不断给函数输入一个句子中的t-1个单词，要求神经网络层自己去不断尝试不同的算法成功预测出"Bert"这个单词，这一层网络学到这个信息后就将训练数据中的词语顺序，不同词汇的前后关联关系以网络特征的方式固定到了这层网络中，之后再根据需要组合更多的网络，这样这个训练好的包含若干层网络的模型就可以用来处理自然语言的任务，模型可以体现出一定的听懂人话，能说人话的能力。
$$
网络层学习公式：W_t=''Bert''
$$

$$
训练公式：P(W_t=''Bert''|W_1,W_2,...W_(t-1);\Theta)
$$

借此算法形成的模型就可以根据用户输入的上下文猜测最可能的连接词，或者进行填空等任务。现在流行的大型语言模型一般可以采用预训练的方式来进行定制化的开发，预训练一般包括两种模式：

+ 一种是浅层加载的参数在训练C任务过程中不动，这种方法被称为“Frozen”;
+ 另外一种是底层网络参数尽管被初始化了，在C任务训练过程中仍然随着训练的进程不断改变，这种一般叫“Fine-Tuning”。

采用预训练调优的方法有一个较大的好处，在于目标领域的训练数据较少时也能进行训练，另外较底层的网络学习到的特征或者知识往往是较为通用的，因此可以用在同类型的任务中作为基础模型来针对性的调优。

GPT-1（Generative Pre-Training）是OpenAI在2018年提出的，采用pre-training和fine-tuning的下游统一框架，将预训练和finetune的结构进行了统一，解决了之前两者分离的使用的不确定性（例如ELMo）。采用的主要方法为对大量文本进行无监督学习，目标函数就是语言模型最大化语句序列出现的概率，其损失函数为：


$$
GPT的损失函数：loss(\Theta)=-\frac{1}{(K_2)}E_(x,y_w,y_l)\sim D[log(\partial (r_\theta(x,y_w)-r_\theta(x,yl)))]
$$
其中，k为上文的窗口，表示参数为的神经网络模型。

表示左侧窗口的上下文词向量，表示Transformer的层数，表示词向量矩阵，表示position embedding矩阵（作者对position embedding矩阵进行随机初始化并训练学习）。

GPT-2验证了扩大无监督训练数据的规模可以在更多场景或领域提升较为明显的有效性；

GPT-3 继续超大幅度的增加训练预料；

## 关于分词

各类可用于分词的库：https://github.com/lonePatient/awesome-pretrained-chinese-nlp-models

| 类型                    | 原始中文分词                       | 分词后                                             |
| ----------------------- | ---------------------------------- | -------------------------------------------------- |
| 传统中文分词（jieba等） | 使用语言模型来预测下一个词的概率。 | 使用 语言 模型 来 预测 下 一个 词 的 概率。        |
| BERT分词                | 使用语言模型来预测下一个词的概率。 | 使 用 语 言 模 型 来 预 测 下 一 个 词 的 概 率 。 |
|                         |                                    |                                                    |

## 向量化Embedding
> + 主要作用是将高维稀疏向量转化为稠密向量，从而方便下游模型处理。
> 
> + 用一个低维稠密向量来表示一个对象，使得这个向量能够表达相应对象的某些特征，同时向量之间的距离能反应对象之间的相似性。
> 
> + Embedding是离散实例连续化的映射。